
AULA PRÉ-PROCESSAMENTO DE DADOS

Dados raramente são perfeitos!

Eles podem ser "sujos", ou seja, podem conter:
 - Valores Ausentes: Campos não preenchidos (ex: um cliente sem idade registrada).
 - Dados Inconsistentes: Erros de digitação ou formatos diferentes (ex: "São Paulo", "SP", "S. Paulo").
 - Outliers: Valores extremos que fogem muito do padrão (ex: uma casa de 20m² custando R$ 5 milhões).
 - Diferentes Escalas: Variáveis com grandezas muito distintas (ex: idade variando de 18 a 80 e salário variando de 1.000 a 50.000).

Os algoritmos de Machine Learning são sensíveis a esses problemas.

PRÉ-PROCESSAMENTO DE DADOS é o conjunto de técnicas aplicadas aos dados brutos para limpá-los, transformá-los e organizá-los em um formato adequado para o treinamento de modelos.

O objetivo é melhorar a qualidade dos dados e a performance e a precisão do modelo.

As etapas mais comuns são:

1 - LIMPEZA DE DADOS: Tratar valores ausentes (removendo ou preenchendo), corrigir inconsistências e remover duplicatas.

2 -TRANSFORMAÇÃO DE DADOS:
   - Normalização/Padronização: Colocar todas as variáveis numéricas na mesma escala.
   - Encoding de Variáveis Categóricas: Converter textos em números, pois os modelos matemáticos não entendem palavras.

3- DIVISÃO DE DADOS (Treino/Teste):
   - Separar o conjunto de dados em duas partes: treinar o modelo (geralmente 80%) e testá-lo (20%), para avaliar se ele consegue generalizar para dados que nunca viu.

Como exemplou, usaremos a biblioteca PANDAS para manipulação de dados e SCIKIT-LEARN para o pré-processamento.

# Importando as bibliotecas
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# 1. Dados Brutos ("sujos")
# Criar DataFrame com Pandas
dados = {
    'tamanho_m2': [50, 70, 100, 120, 65, np.nan, 95],
    'n_quartos': [1, 2, 3, 3, 2, 2, 3],
    'cidade': ['SP', 'RJ', 'SP', 'BH', 'RJ', 'SP', 'BH'],
    'preco': [150000, 210000, 300000, 350000, 190000, 180000, 280000]
}
df = pd.DataFrame(dados)
print("--- Dados Originais ---")
print(df)
print("\nValores ausentes antes do tratamento:")
print(df.isnull().sum())

# 2. Limpeza: Tratar Valores Ausentes
# Usaremos uma estratégia simples: preencher o valor ausente do tamanho com a média dos outros tamanhos.
# 'SimpleImputer' é a ferramenta do scikit-learn para isso.
imputer = SimpleImputer(strategy='mean')
df['tamanho_m2'] = imputer.fit_transform(df[['tamanho_m2']])

print("\n--- Após tratar valores ausentes ---")
print(df)

# 3. Divisão em Treino e Teste
# Separamos as features (X) do nosso alvo (y)
X = df.drop('preco', axis=1) # Todas as colunas, exceto 'preco'
y = df['preco']

# Dividimos os dados: 80% para treino, 20% para teste
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.20, random_state=42)
print(f"\nDividimos os dados em {len(X_treino)} amostras de treino e {len(X_teste)} de teste.")

# 4. Transformação: Encoding e Escalonamento
# Identificamos colunas numéricas e categóricas para aplicar transformações diferentes
colunas_numericas = ['tamanho_m2', 'n_quartos']
colunas_categoricas = ['cidade']

# Para dados categóricos (cidade), usamos OneHotEncoder.
# Ele transforma 'SP', 'RJ', 'BH' em colunas binárias (0 ou 1).
encoder = OneHotEncoder(handle_unknown='ignore')
X_treino_cat = encoder.fit_transform(X_treino[colunas_categoricas]).toarray()
X_teste_cat = encoder.transform(X_teste[colunas_categoricas]).toarray()

# Para dados numéricos, usamos StandardScaler.
# Ele ajusta a média para 0 e o desvio padrão para 1.
scaler = StandardScaler()
X_treino_num = scaler.fit_transform(X_treino[colunas_numericas])
X_teste_num = scaler.transform(X_teste[colunas_numericas])

# Juntamos os dados transformados
X_treino_final = np.hstack([X_treino_num, X_treino_cat])
X_teste_final = np.hstack([X_teste_num, X_teste_cat])

print("\n--- Exemplo de uma linha de dados após pré-processamento ---")
print(X_treino_final[0])
print("\nOs dados estão limpos, transformados e prontos para o modelo!")



Modelos Clássicos: Regressão vs. Classificação
1. Definição Teórica
Em Aprendizado Supervisionado, os dois tipos de problemas mais comuns são Regressão e Classificação.

REGRESSÃO:Usamos quando queremos prever um valor numérico contínuo.
Exemplos de perguntas que a regressão responde:
 - Qual será o preço de uma casa? (Como no nosso exemplo)
 - Qual será a temperatura amanhã?
 - Quantos produtos serão vendidos no próximo mês?
 - Qual a duração de uma viagem de carro?

Algoritmos Comuns: Regressão Linear, Regressão Polinomial, Árvores de Decisão, Random Forest, Gradient Boosting.


CLASSIFICAÇÃO: Usamos quando queremos prever uma categoria ou classe discreta.
Exemplos de perguntas que a classificação responde:
 - Este e-mail é spam ou não spam? (2 classes)
 - Esta foto contém um gato, um cachorro ou um pássaro? (Múltiplas classes)
 - Este cliente vai cancelar a assinatura ou não? (2 classes - problema conhecido como churn)
 - Qual a categoria desta notícia: esportes, política ou tecnologia? (Múltiplas classes)

Algoritmos Comuns: Regressão Logística, KNN, SVM, Árvores de Decisão (para classificação), Naive Bayes.

A GRANDE DIFERENÇA:
 - Se a resposta que você procura é um número em uma escala (preço, idade, quantidade), o problema é de Regressão.
 - Se a resposta é uma etiqueta, um rótulo ou um grupo (sim/não, gato/cachorro, azul/verde/vermelho), o problema é de Classificação.


# --- Exemplo de REGRESSÃO ---
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# 1. Dados: Horas de Estudo vs. Nota no Exame
# X: Horas de Estudo (nossa feature)
# y: Nota Final (nosso alvo numérico)
horas_estudo = np.array([[2], [3], [5], [6], [8], [10], [11], [13]])
nota_exame = np.array([5.5, 6.0, 7.0, 7.2, 8.0, 8.8, 8.9, 9.7])

# 2. Criar e Treinar o Modelo de Regressão
# O modelo vai "aprender" a relação entre horas e notas
modelo_regressao = LinearRegression()
modelo_regressao.fit(horas_estudo, nota_exame)

print("--- Modelo de Regressão (Previsão de Notas) ---")
print("Modelo treinado!")

# 3. Fazer uma Previsão
# Pergunta: Se um aluno estudar 7 horas, qual será sua nota prevista?
horas_novas = np.array([[9]])
nota_prevista = modelo_regressao.predict(horas_novas)
print(f"Previsão para 7 horas de estudo: Nota {nota_prevista[0]:.1f}")

# 4. Visualizar o Resultado
plt.figure(figsize=(10, 6))
# Plota os dados originais (pontos azuis)

plt.scatter(horas_estudo, nota_exame, color='blue', label='Dados Alunos')
# Plota a linha que o modelo aprendeu (linha vermelha)

plt.plot(horas_estudo, modelo_regressao.predict(horas_estudo), color='red', label='Linha de Regressão (Aprendizado do Modelo)')
# Plota a nova previsão (ponto verde)

plt.scatter(horas_novas, nota_prevista, color='green', s=150, zorder=5, label=f'Previsão para {horas_novas[0][0]}h')
plt.title('Regressão: Horas de Estudo vs. Nota do Exame')
plt.xlabel('Horas de Estudo')
plt.ylabel('Nota no Exame (0 a 10)')
plt.legend()
plt.grid(True)
plt.show()


# Exemplo de CLASSIFICAÇÃO
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 1. Dados: Horas de Estudo e Faltas vs. Situação Final
# Features: [Horas de Estudo, Número de Faltas]
# Alvo: 0 para Reprovado, 1 para Aprovado

X_alunos = np.array([
    [2, 8], [4, 5], [5, 2], [6, 4], [7, 1], [8, 0],
    [9, 3], [10, 1], [11, 0], [12, 2]
])
# y: Situação (nosso alvo categórico)
situacao_final = np.array([0, 0, 1, 0, 1, 1, 0, 1, 1, 1]) # 0=Reprovado, 1=Aprovado

# 2. Criar e Treinar o Modelo de Classificação
# O modelo vai aprender a "fronteira" que separa os aprovados dos reprovados
modelo_classificacao = LogisticRegression()
modelo_classificacao.fit(X_alunos, situacao_final)

print("\n--- Modelo de Classificação (Situação do Aluno) ---")
print("Modelo treinado!")

# 3. Fazer uma Previsão
# Pergunta: Um aluno que estudou 7 horas e teve 3 faltas, será aprovado?
aluno_novo = np.array([[7, 3]])
previsao_classe = modelo_classificacao.predict(aluno_novo)
resultado = "Aprovado" if previsao_classe[0] == 1 else "Reprovado"
print(f"Previsão para um aluno com 7h de estudo e 3 faltas: {resultado}")

# 4. Visualizar o Resultado (um pouco mais complexo, mas muito didático)
plt.figure(figsize=(10, 6))
# Plota os alunos reprovados (vermelho) e aprovados (azul)
plt.scatter(X_alunos[situacao_final == 0][:, 0], X_alunos[situacao_final == 0][:, 1], color='red', marker='x', s=100, label='Reprovado')
plt.scatter(X_alunos[situacao_final == 1][:, 0], X_alunos[situacao_final == 1][:, 1], color='blue', marker='o', s=100, label='Aprovado')

# Plota o novo aluno que queremos classificar (ponto verde)
plt.scatter(aluno_novo[:, 0], aluno_novo[:, 1], color='green', marker='*', s=200, zorder=5, label='Novo Aluno')

# Desenha a fronteira de decisão que o modelo aprendeu
x_min, x_max = X_alunos[:, 0].min() - 1, X_alunos[:, 0].max() + 1
y_min, y_max = X_alunos[:, 1].min() - 1, X_alunos[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))
Z = modelo_classificacao.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.2, cmap='coolwarm')

plt.title('Classificação: Aprovado vs. Reprovado')
plt.xlabel('Horas de Estudo')
plt.ylabel('Número de Faltas')
plt.legend()
plt.grid(True)
plt.show()

